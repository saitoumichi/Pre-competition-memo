活性化関数とは：ニューラルネットワークに非線形（真っ直ぐじゃない性質）を与えるための関数
　＝＞ニューラルネットワークを賢くするスイッチ
　活性化関数がない世界：
　各層が線形（足し算・掛け算だけ）だと
　$y = W_3(W_2(W_1 x)) = W x$
　結局1回の計算と同じ
　＝＞深くする意味がない
　活性化関数がある世界：
　途中で非線形を入れると
　・曲がった教会
　・複雑なパターン
　・高度な特徴抽出
　ができる
　＝＞深層学習が成立する
　ニューロンの中で何をしているのか
　・入力 × 重み ＋ バイアス（線形変換）
　・活性化関数に通す
　・出力する
　$y = f(Wx + b)$
  ＝＞どれだけ反応するかを決める関数

代表的な活性化関数（超重要）

### **① ReLU（現在の主流）**
　$f(x) = \max(0, x)$
　- 0以下は0
　- 正の値はそのまま

　**メリット**
　- 計算が速い
　- 勾配消失が起きにくい

　**デメリット**
　- ReLU Dead（ずっと0）

### **② Sigmoid（昔の主流）**
$f(x) = \frac{1}{1 + e^{-x}}$
- 出力：0〜1
- 確率っぽい

**デメリット**
- 勾配消失しやすい
- 深いネットに不向き

### **③ Tanh**
$f(x) = \tanh(x)$
- 出力：-1〜1
- Sigmoidよりは良い

### **④ Softmax（出力層専用）**
$f_i(x) = \frac{e^{x_i}}{\sum_j e^{x_j}}$
- クラス確率を出す
- 合計が1になる

どこで使うか：

| 場所     | 活性化関数      |
| ------ | ---------- |
| 隠れ層    | ReLU（ほぼこれ） |
| 二値分類   | Sigmoid    |
| 他クラス分類 | Softmax    |
| 回帰     | なし（線形）     |
活性化関数が学習に与える影響：
勾配消失問題
・Sigmoid / Tanh
＝＞勾配が小さくなりすぎる
＝＞学習が止まる
ReLUが流行った理由
・勾配が０or１
・深い層まで誤差が伝わる

直感的に：
スイッチ
・入力が弱い＝＞反応しない
・入力が強い＝＞出力する

## **よくある誤解**
❌ 活性化関数は飾り
⭕ **ネットワークの能力を決める核心**

❌ どれでも同じ
⭕ 学習のしやすさが激変