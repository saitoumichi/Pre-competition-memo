**flip の確率を 0.5 → 0.2 に下げる**と、モデルの学習の「性格」が結構はっきり変わります。

---

## **結論を一言で**

- **0.5**：強めの汎化・過学習抑制重視
    
- **0.2**：データ忠実・安定性重視（医療画像ではこちらが合うことも多い）
    

---

## **何がどう変わる？**

  

### **① データ拡張の強さ**

- **p=0.5**
    
    → 2枚に1枚は左右反転
    
    → 「左右どっちでも同じ」という前提を強く学ぶ
    
- **p=0.2**
    
    → 5枚に1枚だけ反転
    
    → 元データの分布をより尊重
    

---

### **② 過学習 vs 安定性**

| **観点**  | **p=0.5** | **p=0.2** |
| ------- | --------- | --------- |
| 過学習抑制   | ◎         | ○         |
| 学習の安定性  | △         | ◎         |
| 元データ忠実度 | △         | ◎         |
- 0.5 は**正則化が強い**
    
- 0.2 は**ノイズが少ない**
    

---

### **③ 医療画像での実務的な影響**

  

医療・病理画像では、左右反転が「完全に同義」とは限らないケースがあります。

- 染色ムラ
    
- 撮影・切り出しの癖
    
- 施設差（スキャナ方向）
    

  

👉 **p=0.5 はやや強すぎることがある**

👉 **p=0.2〜0.3 が無難な落としどころ**になりやすい

---

## **実際に何が起きやすいか（経験則）**

  

### **p = 0.5**

- train acc ↓
    
- val acc ↑（初期）
    
- 収束が遅くなることあり
    
- 小規模データだと不安定
    

  

### **p = 0.2**

- train acc ↑（早く上がる）
    
- val acc が安定
    
- seed 依存が減る
    

---

## **どう書く？**

`transforms.RandomHorizontalFlip(p=0.2),`

---

## **どう判断すべき？**

  

次のどれかに当てはまれば **0.2 推奨**です。

- データ数が少ない
    
- 医療・病理画像
    
- 左右が「だいたい同じ」だが完全ではない
    
- 再現性・安定性を重視したい
    

  

逆に **0.5 のままで良い**のは：

- 大規模データ
    
- 自然画像
    
- 左右反転が完全に同義なタスク
    

---

## **ちょっと上級な考え方（おすすめ）**

  

**epoch が進むにつれて flip を弱める**という設計もあります。

- 前半：p=0.5（汎化を作る）
    
- 後半：p=0.2 or 0.0（分布に寄せる）
    

  

→ 医療系コンペでは割と効きます。

---

## **まとめ**

- p=0.5 → 強い正則化・汎化重視
    
- p=0.2 → 安定・データ忠実・医療向き
    
- **「強すぎたら弱める」判断はとても健全**
    

  

次に考えるなら

[[👉 **RandomRotation を ±30° → ±10° にしたら？**]]

👉 **Flip と Rotation を同時に強くするとどうなる？**