＝＞機械学習の一種で特徴量を人間が設計しなくてもコンピュータが自分で学習できる方法

人間の脳の仕組みを真似たニューラルネットワークを使って、しかもその層（レイヤー）が大量に重なっているため深層（Deep）と呼ばれている

こんな感じ↓

AI
 └─ 機械学習
     └─ 深層学習

AI：データを別のデータに変換する「関数」と考えられる（画像＝＞ラベル、文章＝＞要約みたいな感じ）

機械学習：データからルール学ぶ仕組み
・赤くて丸い＝＞りんご
・細長い＝＞バナナ
（ここでは「特徴」を人間が考えて与えていることが多い）

深層学習：その「特徴を考える部分」もコンピュータ自身が学ぶ

＝＞機械学習と深層学習の違い：

| 観点   | 機械学習     | 深層学習          |
| ---- | -------- | ------------- |
| 特徴量  | 人間が設計    | コンピュータが自動で学習  |
| モデル  | 比較的浅い    | 多層ニューラルネットワーク |
| 得意分野 | 小〜中規模データ | 画像・音声・自然言語    |
＝＞機械学習では人間が特徴を教えているが、深層学習では自分で特徴を見つけ出す

代表例：
・画像分類（犬・猫の判別）
・物体検出・セグメンテーション
　・自動運転の歩行者検出
　・CT・MRI画像から臓器や病変を抽出
・生成系AI
　・GPTみたいな文章生成
　・画像生成・音楽生成

基本的な流れとして：
1. データ準備
　　AIが扱える形に変える
2. データ分割
　　学習用・検証用・テスト用に分ける
　　学習用(train)：重みを学習する＝＞何度も見る
　　検証用(validation)：調整・選択に使う＝＞見るけど学習しない
　　テスト用(test)：最終試験＝＞絶対見せない
3. モデル構築
　　ニューラルネットワークを作る
4. 学習・最適化
　　損失関数を最小化するように重みを更新する
5. 評価
　　どれくらい正確かを数値で確認する

学習用データ（Training data）：
　・重み（パラメータ）を更新するために使う
　・誤差逆伝播で何回も使い回す
　実際の扱い：
　　画像＝＞モデル ＝＞ 予測
　　予測 × 正解 ＝＞ 誤差
　　誤差 ＝＞ 重み更新
　特徴として：
　　・モデルは答えを見ながら学ぶ
　　・何epochも繰り返し使用
　　・一番多い（例：70％）
　＝＞教科書と解答付き問題集で勉強している状態

検証用データ（Validation data）
　・モデルの調整・選択のため
　・学習が「うまくいっているのか」の確認
　実際の扱い：
　　画像＝＞モデル＝＞予測
　　予測 × 正解＝＞精度を計算
　　（重み更新なし）
　ここで決めるもの：
　　・学習を止めるタイミング（過学習防止）
　　・ハイパーパラメータ
　　　・学習率
　　　・層の数
　　　・epoch数など
　特徴として：
　　・正解は「評価のため」に使う
　　・学習には使わない
　　・中くらいの量（例：15％）
　＝＞模試・小テストとかの点数を見て勉強方法を変えるイメージ

テスト用データ（Test data）
　・最終的な実力測定
　・論文・レポート・本番評価に使う
　実際の扱い：
　　画像＝＞モデル＝＞予測
　　予測 × 正解＝＞最終スコア
　絶対ルール：
　　モデル開発中は絶対に見てはいけない
　　・ここ見て調整したらズルになる
　　・見た瞬間「未知データ」ではなくなる
　特徴として：
　　・一度だけ使う
　　・少なめ（例：15％）
　＝＞本番試験でカンニング不可のイメージ

どうして分けているのか
　分けないと起こる地獄
　全部学習に使う
　　・覚えただけ（暗記）
　　・新しいデータで当たらない
　＝＞テストデータで調整するときにテスト結果が信用できなくなるから

過学習（オーバーフィッティング）との関係
　典型パターン：
　
　| epoch | 学習精度 | 検証精度 |
　| ------- | --------- | --------- |
　| 10       | 90%  | 88%  |
　| 50    | 99%  | 85%  |
　
　・学習用は完璧に覚えている
　・検証用では劣化
　＝＞覚えすぎ（過学習）
　＝＞検証用があるからこれに気づける
　実務・検証でよくある割合
　学習用途：60-80%
　検証用途：10-20%
　テスト用途：10-20%
　＝＞データが少ないときは交差検証を使う

重み（パラメータ）とは：
　ニューラルネットワークが「何をどれだけ重要視するか」を表す数値
　一言で言うと：「重み＝入力ごとの重要度（強さ）」
　モデルが学習する正体はほぼこれ
　1. 超シンプルな例
　　　例えばこんな入力があるとすると
　　　$$
　　　 x_1：身長
　　　　$$
　　　$$
　　　x_2：体重
　　　$$
　　　でこの人はスポーツ向きかどうかを判定したい
　　　ニューラルネットワークの中では
　　　$$
　　　　y = w_1 x_1 + w_2 x_2 + b 
　　　$$
　　　　$$
 　　　　w_1：身長をどれくらい重視するか
　　　　　$$
　　　　$$
　　　　 w_2：体重をどれくらい重視するか
　　　　$$
　　　　$$
         　　　　 b：調整用のオフセット（バイアス）　
　　　　$$  
　2. なぜ重みが必要か
 　　もし重みがなかったら：
　　　$$
　　　　y = x_1 + x_2
　　　$$
　　ってなって全部同じ重要度
　　＝＞現実的な判断ができない
　　＝＞重みがあることで
　　・重要な特徴は強調
　　・どうでもいい特徴は弱める
　　ってことができる
   3. ニューラルネットワークでは何が起きているのか
　　　　各ニューロンの中身
　　　　・入力 × 重み
　　　　・全部足す
　　　　・活性化関数に通す
　　   　```
        　`x1 ──× w1 ┐`
        　`x2 ──× w2 ├─ Σ → f() → 出力`
        　`x3 ──× w3 ┘`
      　　```
　　　　＝＞重みが「線の太さ」だと思うとわかりやすい
  4. 学習とは「重みを直すこと」
 　　学習中にやっていることは実はこれだけ：
 　　・予測する
 　　・正解と比べる
 　　・間違い分を計算
 　　・重みを少し動かす
 　　これを何回も繰り返す
 　　重要ポイント：
 　　・モデルは「ルール」を覚えていない
 　　・数値（重み）だけを調整している
　5. パラメータとは
 　　　用語として：
 　　　・重み（Weight）：学習される数値
　　　　・バイアス（Bias）：学習される補正値
　　　　・パラメータ：重み＋バイアスの総称
　　　＝＞パラメータ＝モデルの中身そのもの
　　　＝＞ソフトウェアを実行したりプログラム内で関数を呼び出したりするときに一連の処理を指定するために与える情報
　6. どうして大量のデータが必要なのか
　　　　パラメータが多いほど
　　　　・表現力が上がる
　　　　・でも適当に決めるとだめ
　　　　＝＞たくさんのデータで少しずつ正しい値に近づける必要がある
　7. 超重要な誤解を潰す
　　　　❌「AIが意味を理解している」
　　　　⭕「重みの組み合わせがうまく調整されているだけ」
　　　　AIは、
　　　　「猫」を知っているわけではない
　　　　「猫っぽい数値のパターン」に反応している
誤差逆伝播とは：
　**ニューラルネットワークが「どの重みを、どれくらい直せば誤差が減るか」を計算する方法**
　一言で言うと：
　出力の間違いを出力層＝＞入力層へ逆向きに伝えて書く重みの”責任の大きさ”を計算する仕組み
　なぜ必要か：
　　ニューラルネットワークには重みが大量にある（全部ちょっとずつ直すだけではダメ）
　　・どの重みが悪さをしたか
　　・どれくらい動かせば良いか
　　を数学的に正確に知る必要がある
　　それを可能にするためのやつが誤差逆伝播法
　全体の流れ：
　　・順伝播（Forward）
　　　入力＝＞出力を計算（予測）
　　・誤差計算
　　　予測と正解の差（損失）を計算
　　・逆伝播
　　　誤差を後ろから前へ伝える
　　・重み更新
　　　誤差が減る方向に重みを少し動かす
　　具体例：
　　１層のモデル
　　　　y = w x
　　　・正解：10
　　　・予測：8
　　　・誤差：L＝(y-10)^2
　　　問題＝＞wを増やすか、減らすか、どれくらいか
　　　誤差伝播の答えとして：
　　　「誤差がwを少し変えたときにどれだけ変わるか」を見る
　　　　$$
　　　　　数学的には：\frac{\partial L}{\partial w}
　　　　$$
　　　　・プラス＝＞w減らす
　　　　・マイナス＝＞w増やす
　　　　・大きい＝＞たくさん直す
　　　　・小さい＝＞少し直す
　　　　＝＞これが勾配
　　多層になると
　　　問題として：
　　　・出力の誤差はわかる
　　　・でも奥の層の深みがでどれだけ悪いかわからない
　　　解決策として：
　　　連鎖律（チェーンルール）
　　　「後ろの影響」×「自分の影響」
　　　で順に伝えていく
　　　出力層＜＝隠れ層＜＝入力層
　　　＝＞これが逆向きに誤差を流すって意味になる
　　　直感的に：
　　　・料理がまずい（誤差）
　　　・塩？砂糖？火加減？
　　　・「どれがどれくらい原因か」を逆算する
　　　＝＞それぞれの責任割合を計算するのが誤差伝播法
　　数式を一切使わない理解
　　・出力が間違ったか
　　・直前の層はどれくらい影響したか
　　・さらにその前はどうなのか
　　＝＞これを数値で正確に伝言ゲームしているだけだから
　　勾配降下法との関係
　　・誤差逆伝播法＝＞どっちに動かすかを計算
　　・勾配降下法＝＞実際に動かして計算
　　のセットで使われている　　



ニューラルネットワークとは：人間の脳（ニューロン）の情報処理をヒントに作られた「関数の集まり」
＝＞深層学習の中身そのもの
　一言でいうと：
　入力＝＞重み付き計算＝＞出力
　をたくさん重ねて「複雑な変換」をできるようにした仕組み
　　9. ニューロン（最小単位）で何をしているのか
　　　処理の流れ
　　　・入力を受け取る（数値）
　　　・重みを掛ける
　　　・足し算をする
　　　・活性化関数に通す
　　　・出力する
　　　数式で書くと：
　　　y = f(w_1x_1 + w_2x_2 + \dots + b)
　　　- x：入力
　　　- w：重み（学習される）
　　　- b：バイアス
　　　- f：活性化関数
　　　👉 **「入力をどう強く見るか」を決めているのが重み**
　　2. 層（レイヤー）を重ねると何が起こるか
　　　　基本構造
　　　　```
　　　　入力層＝＞隠れ層＝＞出力層
　　　　```
　　　　・入力層：データそのもの（画像の画素、数値など）
　　　　・隠れ層：特徴を抽出する層
　　　　・出力層：予測結果
　　　　隠れ層が必要な理由：
　　　　層を重ねることで
　　　　・１層目：単純な特徴
　　　　・２層目：特徴の組み合わせ
　　　　・３層目：より抽象的な概念
　　　　と言うふうにだんだん意味のある表現を作ることができるお
　　3. 深層学習との関係
　　　ニューラルネットワーク：構造・仕組み＝＞層が浅い
　　　深層学習：ニューラルネットワークを多層にした学習方法＝＞層が深い
　　4. どうやって学習するのか
　　　予測が外れた分だけ重みを少しずつ直している
　　　流れとして：
　　　・入力を入れて予測する
　　　・正解と比べて誤差（損失）を計算
　　　・誤差が小さくなる方向に重みを調整
　　　・これを何回も繰り返す
　　　この重み調整を行う仕組みが誤差逆伝播法（バックプロパゲーション）
　　　バックプロパゲーションとは：
　　5. 活性化関数って何のためにあるのか
　　活性化関数がないとどれだけ層を重ねても：
　　ただの一次関数の合成＝＞表現力が弱い
　　よく使われる例（活性化関数）：
　　- ReLU：今の主流（速くて強い）
　　- Sigmoid：昔よく使われた
　　- Softmax：分類の出力層
　　👉 **「非線形」を入れるために必須**
　　ReLUとは：
　　Sigmoidとは：
　　Softmaxとは：
　　非線形とは：
　6. なぜ画像・音声・言語が得意なのか
　　　・人間が「特徴」 を設計しなくて良い
　　　・データが多いほど勝手に賢くなる
　　　・層が深いほど抽象化が進む
　　　だから：
　　　・画像＝＞CNN
　　　・文章＝＞RNN / Transform
　　　など用途別NNが生まれた
　　
