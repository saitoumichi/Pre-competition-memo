**Transformer（トランスフォーマー）**とは、
**「注意（Attention）」だけを使ってデータ同士の関係を一気に捉える深層学習モデル**です。
今の **ChatGPT・翻訳・要約・画像生成**の基盤になっています。
## **一言でいうと**

> **Transformer =**> **「全部を同時に見渡して、重要な関係にだけ注目するモデル」**

## **なぜTransformerが登場した？**
それまで主流だった **RNN / LSTM** には問題がありました。


| 問題    | 内容         |
| ----- | ---------- |
| 長文苦手  | 前の情報を忘れやすい |
| 遅い    | １単語ずつ順序処理  |
| 並列化不可 | GPUを活かしにくい |
＝＞Transformerで解決

![[f033cd767496ca2b6553d0b6ee12829f26729327c787be4756d6b1ee69405ccd.png]]

![[3445f5a204dfe25a58a5c924d980db5bfb4c45dca89ee7685f0d0d9a28ecc9b1.png]]

![[8854e2d8cf388d99176c48bdd8656b273f15bfe2481ded79be04b204d67ca1b0.jpg]]

入力
 ↓
埋め込み + 位置情報
 ↓
Self-Attention
 ↓
Feed Forward
 ↓
（何層も重ねる）
 ↓
出力

## **核心①：Self-Attention（自己注意）**
### **これがTransformerの本体**
文章：
> 「私は 昨日 図書館で 本を 読んだ」
「読んだ」は
- **誰が？ → 私**
- **何を？ → 本**
- **どこで？ → 図書館**

👉 **全部の単語同士の関係を一気に計算**

---

### **仕組み（ざっくり）**
各単語から
- **Query（質問）**
- **Key（鍵）**
- **Value（値）**
を作り、
> 「この単語にとって、どの単語がどれくらい重要か？」
を数値化します。

---

## **核心②：並列処理できる**

- RNN：
`私 → 昨日 → 図書館で → 本を → 読んだ`
- Transformer：
`私 昨日 図書館で 本を 読んだ（全部同時）`
＝＞圧倒的に速い


